---

*History*

Twenty years ago, Nvidia was squarely focused on one thing: 3D graphics.

Not long from now, its founder and CEO Jen-Hsun Huang expects to preside over a company that reaches into every corner of the computing world thanks to a financial war chest the company built up from a strategy that saw its graphics processing units (GPUs) become the work horses of artificial intelligence (AI) in data centres.

Alongside the ability to handle many floating-point operations in parallel – an attribute needed for training deep neural networks – the main reason Nvidia's GPUs became more commonly used in data centres than competitors such as AMD lay in its CUDA environment. This is a strategy Nvidia aims to repeat following its acquisition of networking specialist Mellanox, coupled with some rebranding before it rounds out the portfolio with the addition of Arm and its general-purpose processor architectures.



*MIT study about Deep Learning approaching computation limits.*


Deep learning’s recent history has been one of achievement: from triumphing
over humans in the game of Go to world-leading performance in image recognition, voice recognition, translation, and other tasks. But this progress has
come with a voracious appetite for computing power. 
MIT study focuses on
the computational demands of Deep Learning applications in five prominent
application areas and shows that progress in all five is strongly reliant on increases in computing power. Extrapolating forward this reliance reveals that
progress along current lines is rapidly becoming economically, technically, and
environmentally unsustainable. Thus, continued progress in these applications
will require dramatically more computationally-efficient methods, which will
either have to come from changes to deep learning or from moving to other
machine learning methods.

*#Solution:# Domain Specific Accelerators that can also reduce energy consumption. That’s the reason we see more and more specialized architecture for deep learning models.*

Full article:
link:https://arxiv.org/pdf/2007.05558.pdf[]



*MLPerf*

MLPerf, an organization that’s developing standards for hardware performance in machine learning tasks - and it will be mostly used in this report.

MLPerf measures how long it takes various hardware configurations to train particular machine learning models. 

Tasks include object detection, image classification, language translation, recommendation, and reinforcement learning goals.


Latest results shows:

- Systems from nine organizations trained models 2.7 times faster, on average, than they did in tests conducted last November, demonstrating the rapid evolution of AI hardware (and enabling software such as compilers).
- Nvidia submitted 40 different configurations. Those based on its A100 graphics processing unit (GPU) scored highest among commercially available systems.
- Showing off capabilities that aren’t yet on the market, Google dominated six of the eight tasks with its fourth-generation tensor processing unit (TPU). Earlier versions are available via the Google Cloud platform.
- Alibaba, Fujitsu, Intel, Inspur, Shenzhen Institute, and Tencent also joined the competition. AI hardware upstarts Cerebras and Graphcore are “New Horsepower for Neural Nets”.




---

*Technical note*

Currently there are 6 main areas:

1. *CPU* - advances in CPUs (including server, mobile, and edge).
2. *GPU* - standard GPUs and specialized one - like cerberas wafer.
3. *NPU / TPU / DPU / IPU / xPU* - all ASIC type solutions - specialized hardware.
4. *FPGA* - re-programmable hardware.
5. *SoC* - advanced CPUs, that contains not only GPUs, but also parts of FPGAs/xPUs - ie: Apple M1 chip.
6. *Quantum* - this is quite futuristic as of today - but it grows, and have some hype, so worth to have some initial insight, and be something to follow up.

There are companies that has products in more than one area (IBM,Intel,AMD,nVidia ..) so you could see it's name/products distributed among those areas.

There are products that could be categorized in more than one area since there is thin line between ie: CPU or SoC - so there could be some confusions - I tried to put them into the category where I feel it belongs (at least at current stage).

In future I may reorganize report to be on company-by-bompany basis.

NOTE: CPUs is here to have general/complete view, at the end they are not accelerators.

---

*Structure*

Each chapter contains 3 sections:

1.
Introduction to company/products in form on NOTE. 

Example:
[NOTE]
====
Short summary about company and latest product/development - few bullet points.

Website to follow up, ie: link:http://yarenty.com[]
====

2. 
Extract from website / news - latest product details, ie: 

- hardware description, 
- software capabilities, 
- benchmarking - MLPerf output.

Content presented is mostly about latest development - latest chip, or latest version of software - at the end this is all about technology trends not full history of each company.


3.
Third seciton is ... note from reporter ;-) - short investigation about what was going on lately in the company, ie: last year activity / development / rumours / are they growing or stagnating. 

Example:
[IMPORTANT]
.Note from Jaro
====
Hardware Acceleration is Booming ;-)
====







 